{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7d89f5",
   "metadata": {},
   "source": [
    "\n",
    "# Add-to-Bag \n",
    "\n",
    "This notebook follows the strategy you outlined:\n",
    "\n",
    "1) **Data Collection & Feature Engineering** — user behavior, item attributes, user history (from available fields), time & query context, and smoothed popularity histories (product/brand/type/color/user).  \n",
    "2) **Model Choice** — Random Forest, LightGBM, and XGBoost.  \n",
    "3) **Training & Prediction** — time-based split (train ≤ 2010‑01‑05, validate 2010‑01‑06..07), probability outputs.  \n",
    "4) **Evaluation** — PR‑AUC, LogLoss, ROC‑AUC (optional), and ranking metrics: MAP@10, NDCG@10, HR@10.  \n",
    "5) **Deployment** — retrain on full train (2010‑01‑01..07), score holdout (2010‑01‑08), save `predictions.csv` with required columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f98be521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAS_LGB: True HAS_XGB: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import average_precision_score, log_loss, roc_auc_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional boosters (install if missing)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except Exception:\n",
    "    HAS_LGB = False\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "print(\"HAS_LGB:\", HAS_LGB, \"HAS_XGB:\", HAS_XGB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018eec9b",
   "metadata": {},
   "source": [
    "## Input (adjust paths if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726781c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4974954 entries, 0 to 4974953\n",
      "Data columns (total 8 columns):\n",
      " #   Column                     Dtype         \n",
      "---  ------                     -----         \n",
      " 0   search_query_time          datetime64[ns]\n",
      " 1   user_id                    int32         \n",
      " 2   search_query_id            int32         \n",
      " 3   product_id                 int32         \n",
      " 4   rank                       int32         \n",
      " 5   price_discount_percentage  float64       \n",
      " 6   viewed                     bool          \n",
      " 7   added_to_bag               bool          \n",
      "dtypes: bool(2), datetime64[ns](1), float64(1), int32(4)\n",
      "memory usage: 161.3 MB\n",
      "\n",
      "Holdout shape: (567384, 8)\n",
      "\n",
      "Products shape: (175934, 4)\n"
     ]
    }
   ],
   "source": [
    "# Define file paths for the three parquet datasets\n",
    "TRAIN_PATH   = \"interactions_train.parquet\"\n",
    "HOLDOUT_PATH = \"interactions_holdout_predictions.parquet\"\n",
    "PRODUCTS_PATH = \"products.parquet\"\n",
    "\n",
    "# Load parquet files into pandas DataFrames\n",
    "train = pd.read_parquet(TRAIN_PATH)\n",
    "holdout = pd.read_parquet(HOLDOUT_PATH)\n",
    "products = pd.read_parquet(PRODUCTS_PATH)\n",
    "\n",
    "# Inspect the training dataset\n",
    "train.info(memory_usage='deep')\n",
    "\n",
    "# Print the shapes of holdout and products datasets\n",
    "print(\"\\nHoldout shape:\", holdout.shape)\n",
    "print(\"\\nProducts shape:\", products.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9cd3d",
   "metadata": {},
   "source": [
    "## Time-based split (train ≤ Jan 5, validate Jan 6–7)\n",
    "\n",
    "We split the dataset by time to mimic the real-world scenario: training on past interactions (Jan 1–5) and validating on future interactions (Jan 6–7). This prevents data leakage, ensures evaluation is realistic, and tests whether the model generalizes to unseen days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8bca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_fit rows: 3558753  | Val rows: 679725\n"
     ]
    }
   ],
   "source": [
    "# Define the validation window\n",
    "val_start = pd.Timestamp(\"2010-01-06\")\n",
    "val_end   = pd.Timestamp(\"2010-01-07\")\n",
    "\n",
    "# Create training subset\n",
    "train_fit = train[train[\"search_query_time\"] < val_start].copy()\n",
    "\n",
    "# Create validation subset\n",
    "val_df    = train[(train[\"search_query_time\"] >= val_start) & (train[\"search_query_time\"] <= val_end)].copy()\n",
    "\n",
    "# Print row counts for sanity check\n",
    "print(\"Train_fit rows:\", len(train_fit), \" | Val rows:\", len(val_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2268491",
   "metadata": {},
   "source": [
    "## Helpers — ID normalization, metrics, features, smoothed histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_ids(df):\n",
    "    \"\"\"\n",
    "    Ensure consistency of ID columns by converting them to numeric int64.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing ID columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Copy of the DataFrame with normalized ID columns.\n",
    "    \"\"\"\n",
    "    for c in [\"product_id\",\"brand_id\",\"product_type_id\",\"colour_id\",\"user_id\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(-1).astype(\"int64\")\n",
    "    return df\n",
    "\n",
    "# Ranking metric: Average Precision at K (AP@K)\n",
    "def ap_at_k(group, k=10):\n",
    "    \"\"\"\n",
    "    Compute the Average Precision at K (AP@K) for a single ranked group.\n",
    "\n",
    "    AP@K measures the quality of the ranking by averaging precision\n",
    "    values at the positions of relevant items, up to the top-k results.\n",
    "    It rewards placing positive examples (y_true=1) earlier in the list.\n",
    "\n",
    "    Args:\n",
    "        group (pd.DataFrame): Subset of rows belonging to a single query\n",
    "            or session, containing at least:\n",
    "              - 'y_true': ground-truth labels (0 or 1)\n",
    "              - 'y_score': model scores or predicted probabilities\n",
    "        k (int, optional): Cutoff rank (default=10). Only the top-k\n",
    "            items are considered when computing AP.\n",
    "\n",
    "    Returns:\n",
    "        float: Average precision score for this group at cutoff k.\n",
    "    \"\"\"\n",
    "    # Sort rows by predicted score (highest first) and keep only top-k\n",
    "    g = group.sort_values(\"y_score\", ascending=False).head(k)\n",
    "\n",
    "    # Extract true labels for these k items\n",
    "    y = g[\"y_true\"].to_numpy()\n",
    "\n",
    "    # If there are no positives in this group, AP is defined as 0\n",
    "    if y.sum()==0: return 0.0\n",
    "    precisions, hits = [], 0\n",
    "\n",
    "    # Iterate through each rank position (1..k)\n",
    "    for i, t in enumerate(y, 1):\n",
    "        if t == 1:\n",
    "            hits += 1\n",
    "            precisions.append(hits / i)\n",
    "    return float(np.mean(precisions)) if precisions else 0.0\n",
    "\n",
    "# Ranking metric: Normalized Discounted Cumulative Gain at K (NDCG@K)\n",
    "def ndcg_at_k(group, k=10):\n",
    "    \"\"\"\n",
    "    Compute Normalized Discounted Cumulative Gain (NDCG) at cutoff K\n",
    "    for a single ranked group.\n",
    "\n",
    "    NDCG@K evaluates how well the model ranks relevant items in the\n",
    "    top-K results. It rewards placing relevant items earlier in the list\n",
    "    by applying a logarithmic discount to lower-ranked positions.\n",
    "\n",
    "    Args:\n",
    "        group (pd.DataFrame): Subset of rows belonging to a single query\n",
    "            or session, containing at least:\n",
    "              - 'y_true': ground-truth labels (0 or 1)\n",
    "              - 'y_score': model scores or predicted probabilities\n",
    "        k (int, optional): Cutoff rank (default=10). Only the top-k\n",
    "            items are considered in the calculation.\n",
    "\n",
    "    Returns:\n",
    "        float: NDCG score between 0 and 1 for this group at cutoff k.\n",
    "               1.0 indicates perfect ranking of relevant items.\n",
    "    \"\"\"\n",
    "    g = group.sort_values(\"y_score\", ascending=False).head(k)\n",
    "    y = g[\"y_true\"].to_numpy()\n",
    "\n",
    "    # If there are no positives in this group, NDCG is defined as 0\n",
    "    if y.sum()==0: return 0.0\n",
    "\n",
    "    # Compute logarithmic discount factors: 1/log2(rank+1)\n",
    "    discounts = 1.0 / np.log2(np.arange(2, len(y)+2))\n",
    "\n",
    "    # DCG: sum of discounted gains for predicted ranking\n",
    "    dcg  = float((y * discounts).sum())\n",
    "\n",
    "    # IDCG: ideal DCG if all positives were ranked at the top\n",
    "    idcg = float((np.sort(y)[::-1] * discounts).sum())\n",
    "\n",
    "    # Normalize: NDCG = DCG / IDCG\n",
    "    return dcg/idcg if idcg>0 else 0.0\n",
    "\n",
    "# Ranking metric: Hit Ratio at K (HR@K)\n",
    "def hr_at_k(group, k=10):\n",
    "    \"\"\"\n",
    "    Compute the Hit Ratio at cutoff K (HR@K) for a single ranked group.\n",
    "\n",
    "    HR@K measures whether at least one relevant item appears in the\n",
    "    top-K results for a query/session. It is a binary metric per group:\n",
    "    - Returns 1.0 if there is at least one positive label in the top-K\n",
    "    - Returns 0.0 otherwise\n",
    "\n",
    "    Args:\n",
    "        group (pd.DataFrame): Subset of rows for a single query/session,\n",
    "            containing at least:\n",
    "              - 'y_true': ground-truth labels (0 or 1)\n",
    "              - 'y_score': model scores or predicted probabilities\n",
    "        k (int, optional): Cutoff rank (default=10).\n",
    "                           Only the top-K items are checked.\n",
    "\n",
    "    Returns:\n",
    "        float: 1.0 if at least one relevant item is in top-K, else 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort rows by predicted score (highest first) and keep only top-k\n",
    "    g = group.sort_values(\"y_score\", ascending=False).head(k)\n",
    "\n",
    "    # If any relevant item (y_true=1) exists in the top-k, it's a hit\n",
    "    return 1.0 if g[\"y_true\"].sum() > 0 else 0.0\n",
    "\n",
    "# Feature Engineering: Query-level, Time-based, and Discount Features\n",
    "def add_basic_feats(d):\n",
    "    \"\"\"\n",
    "    Enrich a dataframe of search interactions with basic contextual features.\n",
    "\n",
    "    This function creates features that describe the search query context,\n",
    "    temporal information, and relative discount signals. All features are\n",
    "    safe to compute at inference time (no label leakage).\n",
    "\n",
    "    Args:\n",
    "        d (pd.DataFrame): Input interactions dataframe containing at least:\n",
    "            - 'search_query_id' (int)\n",
    "            - 'rank' (int)\n",
    "            - 'search_query_time' (datetime64)\n",
    "            - 'price_discount_percentage' (float)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Copy of input dataframe with new feature columns added.\n",
    "    \"\"\"\n",
    "    d = d.copy()\n",
    "\n",
    "    # results_per_query: how many products were shown in this query\n",
    "    qsize = d.groupby(\"search_query_id\").size().rename(\"results_per_query\").astype(\"int32\")\n",
    "    d[\"results_per_query\"] = d[\"search_query_id\"].map(qsize)\n",
    "\n",
    "    # rank_norm: normalize rank by query size (0 = top, 1 = bottom)\n",
    "    d[\"rank_norm\"] = d[\"rank\"] / d[\"results_per_query\"].clip(lower=1)\n",
    "\n",
    "    # Time-based features\n",
    "    d[\"hour\"] = d[\"search_query_time\"].dt.hour.astype(\"int16\")\n",
    "    d[\"dow\"]  = d[\"search_query_time\"].dt.dayofweek.astype(\"int16\")\n",
    "\n",
    "    # Query discount statistics\n",
    "    q_disc = d.groupby(\"search_query_id\")[\"price_discount_percentage\"].agg([\"mean\",\"std\"]).rename(\n",
    "        columns={\"mean\":\"q_disc_mean\",\"std\":\"q_disc_std\"}\n",
    "    )\n",
    "\n",
    "    # Map mean and std discount back to each row\n",
    "    d[\"q_disc_mean\"] = d[\"search_query_id\"].map(q_disc[\"q_disc_mean\"])\n",
    "    d[\"q_disc_std\"]  = d[\"search_query_id\"].map(q_disc[\"q_disc_std\"]).fillna(0.0)\n",
    "    \n",
    "    # Session-ish relative features\n",
    "    d[\"rank_pct\"] = d.groupby(\"search_query_id\")[\"rank\"].rank(pct=True).astype(\"float32\")\n",
    "    d[\"disc_rel\"] = (d[\"price_discount_percentage\"] - d.groupby(\"search_query_id\")[\"price_discount_percentage\"].transform(\"mean\")).astype(\"float32\")\n",
    "    return d\n",
    "\n",
    "# Smoothed historical add-rates (fit on past only)\n",
    "def smoothed_histories(df_fit, global_add, alpha=5.0, keys=(\"product_id\",\"brand_id\",\"product_type_id\",\"colour_id\",\"user_id\")):\n",
    "    \"\"\"\n",
    "    Compute smoothed historical add-to-bag rates for specified keys.\n",
    "\n",
    "    This function calculates, for each entity type (e.g., product, brand),\n",
    "    the probability that an item is added to bag given it was displayed.\n",
    "    Rates are smoothed with a Bayesian prior to avoid extreme values for\n",
    "    rare entities.\n",
    "\n",
    "    Args:\n",
    "        df_fit (pd.DataFrame): Training subset used to compute histories.\n",
    "                               Must contain 'added_to_bag' and the keys.\n",
    "        global_add (float): Global add-to-bag rate, used as prior mean.\n",
    "        alpha (float, optional): Smoothing factor; higher values pull\n",
    "                                 entity rates closer to the global rate.\n",
    "                                 Default is 5.0.\n",
    "        keys (tuple of str, optional): Column names to compute histories for.\n",
    "                                       Defaults to ('product_id', 'brand_id',\n",
    "                                       'product_type_id', 'colour_id', 'user_id').\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping {key_name: DataFrame} where each DataFrame contains:\n",
    "              - key column (e.g., 'product_id')\n",
    "              - smoothed add-rate column (e.g., 'product_id_add_rate')\n",
    "    \"\"\"\n",
    "    tables = {}\n",
    "    for key in keys:\n",
    "        if key not in df_fit.columns: \n",
    "            # Skip keys not present in the dataset\n",
    "            continue\n",
    "\n",
    "        # Aggregate counts per entity: how many adds, how many exposures\n",
    "        g = df_fit.groupby(key).agg(adds=(\"added_to_bag\",\"sum\"),\n",
    "                                    n=(\"added_to_bag\",\"size\")).reset_index()\n",
    "        \n",
    "        # Apply Bayesian smoothing with global prior\n",
    "        g[f\"{key}_add_rate\"] = (g[\"adds\"] + alpha*global_add) / (g[\"n\"] + alpha)\n",
    "\n",
    "        # Keep only the identifier and its computed rate\n",
    "        tables[key] = g[[key, f\"{key}_add_rate\"]]\n",
    "    return tables\n",
    "\n",
    "# Merge Smoothed History Tables Back into Main Dataset\n",
    "def merge_histories(df, tables, fill_val):\n",
    "    \"\"\"\n",
    "    Enrich a DataFrame with smoothed add-to-bag history features.\n",
    "\n",
    "    This function merges precomputed historical add-rates (from\n",
    "    `smoothed_histories`) back onto the main interactions DataFrame\n",
    "    for each specified key (e.g., product_id, brand_id).\n",
    "\n",
    "    Any missing values (e.g., new IDs unseen in training) are filled\n",
    "    with a global prior rate to ensure consistent coverage.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Main DataFrame of interactions. Must contain\n",
    "                           the identifier columns present in `tables`.\n",
    "        tables (dict): Dictionary of lookup DataFrames as returned by\n",
    "                       `smoothed_histories`, keyed by column name.\n",
    "                       Each table should contain:\n",
    "                         - the key column (e.g., \"product_id\")\n",
    "                         - a corresponding add-rate column (e.g.,\n",
    "                           \"product_id_add_rate\")\n",
    "        fill_val (float): Value to fill in for missing add-rates\n",
    "                          (typically the global add-to-bag rate).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Copy of input DataFrame with additional\n",
    "                      `*_add_rate` columns merged in.\n",
    "    \"\"\"\n",
    "    d = df.copy()\n",
    "\n",
    "    # Iterate through each entity type and merge its add-rate features\n",
    "    for key, t in tables.items():\n",
    "        # Left join ensures all rows in d are preserved\n",
    "        d = d.merge(t, on=key, how=\"left\")\n",
    "        # Replace NaNs (IDs unseen in training) with global prior rate\n",
    "        d[f\"{key}_add_rate\"] = d[f\"{key}_add_rate\"].fillna(fill_val)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a928247",
   "metadata": {},
   "source": [
    "## Join product attributes & build features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7fce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (3558753, 18)   X_val: (679725, 18)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Join products\n",
    "train_fit = train_fit.merge(products, on=\"product_id\", how=\"left\")\n",
    "val_df    = val_df.merge(products,    on=\"product_id\", how=\"left\")\n",
    "holdout_df   = holdout.merge(products,   on=\"product_id\", how=\"left\")\n",
    "\n",
    "# Normalize IDs\n",
    "train_fit = normalize_ids(train_fit)\n",
    "val_df    = normalize_ids(val_df)\n",
    "holdout_df   = normalize_ids(holdout_df)\n",
    "\n",
    "# Add safe features\n",
    "train_fit = add_basic_feats(train_fit)\n",
    "val_df    = add_basic_feats(val_df)\n",
    "holdout_df   = add_basic_feats(holdout_df)\n",
    "\n",
    "# Histories from train_fit only\n",
    "global_add = train_fit[\"added_to_bag\"].mean()\n",
    "H = smoothed_histories(train_fit, global_add, alpha=5.0)\n",
    "\n",
    "train_fit = merge_histories(train_fit, H, fill_val=global_add)\n",
    "val_df    = merge_histories(val_df,    H, fill_val=global_add)\n",
    "holdout_df   = merge_histories(holdout_df,   H, fill_val=global_add)\n",
    "\n",
    "# Feature set\n",
    "FEATS = [\n",
    "    \"rank\",\"rank_norm\",\"results_per_query\",\"rank_pct\",\n",
    "    \"hour\",\"dow\",\n",
    "    \"price_discount_percentage\",\"q_disc_mean\",\"q_disc_std\",\"disc_rel\",\n",
    "    \"product_id_add_rate\",\"brand_id_add_rate\",\"product_type_id_add_rate\",\"colour_id_add_rate\",\"user_id_add_rate\",\n",
    "    \"product_type_id\",\"brand_id\",\"colour_id\"\n",
    "]\n",
    "\n",
    "X_train = train_fit[FEATS]; y_train = train_fit[\"added_to_bag\"].astype(int)\n",
    "X_val   = val_df[FEATS];    y_val   = val_df[\"added_to_bag\"].astype(int)\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"  X_val:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1813758",
   "metadata": {},
   "source": [
    "## Training & evaluation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835a223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight: 670.8431187464603\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation Helper\n",
    "def evaluate_probs(name, probs, y_val, val_df): \n",
    "    \"\"\"\n",
    "    Evaluate a model's probability predictions on validation data.\n",
    "\n",
    "    Computes both probability-based metrics and ranking-based metrics,\n",
    "    then prints a concise summary and returns results in a dictionary.\n",
    "\n",
    "    Metrics computed:\n",
    "        - PR-AUC : Area under the Precision-Recall curve (good for imbalance)\n",
    "        - LogLoss: Penalizes poorly calibrated probabilities\n",
    "        - MAP@10 : Mean Average Precision at cutoff 10, averaged per query\n",
    "        - NDCG@10: Normalized Discounted Cumulative Gain at cutoff 10\n",
    "        - HR@10  : Hit Ratio at cutoff 10\n",
    "\n",
    "    Args:\n",
    "        name (str): Label/name of the model (for reporting).\n",
    "        probs (array-like): Predicted probabilities for the positive class.\n",
    "        y_val (array-like): Ground-truth binary labels (0 or 1).\n",
    "        val_df (pd.DataFrame): Validation dataframe containing\n",
    "                               at least 'search_query_id' for grouping.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with model name and all metric values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Build evaluation DataFrame with required fields\n",
    "    ve = pd.DataFrame({\n",
    "        \"search_query_id\": val_df[\"search_query_id\"].values,\n",
    "        \"y_true\": y_val.values,\n",
    "        \"y_score\": probs\n",
    "    })\n",
    "\n",
    "    # Compute ranking metrics \n",
    "    map10  = ve.groupby(\"search_query_id\").apply(ap_at_k,  10).mean()\n",
    "    ndcg10 = ve.groupby(\"search_query_id\").apply(ndcg_at_k, 10).mean()\n",
    "    hr10   = ve.groupby(\"search_query_id\").apply(hr_at_k,  10).mean()\n",
    "\n",
    "    # Probability-based metrics\n",
    "    prauc  = average_precision_score(y_val, probs)\n",
    "    ll     = log_loss(y_val, np.clip(probs, 1e-9, 1-1e-9))\n",
    "\n",
    "    # Print formatted one-line summary (easy to compare models)\n",
    "    print(f\"[{name}] PR-AUC={prauc:.6f}  LogLoss={ll:.6f}  MAP@10={map10:.6f}  NDCG@10={ndcg10:.6f}  HR@10={hr10:.6f}\")\n",
    "\n",
    "    # Return metrics as a dict for leaderboard aggregation\n",
    "    return dict(model=name, pr_auc=prauc, logloss=ll, map10=map10, ndcg10=ndcg10, hr10=hr10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d74e736",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "I selected Random Forest as a robust baseline, and LightGBM and XGBoost as state-of-the-art gradient boosting models well-suited for large, imbalanced tabular data. This combination balances interpretability, scalability, and predictive power, ensuring we can benchmark simple and advanced models fairly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb6677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight: 670.8431187464603\n"
     ]
    }
   ],
   "source": [
    "# Compute class imbalance ratio for tree-based models\n",
    "results = [] # container to store evaluation metrics for each model\n",
    "\n",
    "# Count positives (label=1) and negatives (label=0) in the training set\n",
    "pos = int(y_train.sum()); neg = int((y_train==0).sum())\n",
    "spw = neg / max(1, pos)\n",
    "print(\"scale_pos_weight:\", spw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8f674",
   "metadata": {},
   "source": [
    "### Model 1 — Random Forest (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e952b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RandomForest] PR-AUC=0.022005  LogLoss=0.028168  MAP@10=0.028708  NDCG@10=0.033864  HR@10=0.049992\n"
     ]
    }
   ],
   "source": [
    "# Initialize RandomForestClassifier\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=400, max_depth=None, min_samples_leaf=2,\n",
    "    class_weight=\"balanced\", n_jobs=-1, random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the training subset\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the validation set\n",
    "probs_rf = rf.predict_proba(X_val)[:,1]\n",
    "\n",
    "# Evaluate model performance on ranking + probability metrics and append results to the leaderboard list\n",
    "results.append(evaluate_probs(\"RandomForest\", probs_rf, y_val, val_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6264b7",
   "metadata": {},
   "source": [
    "### Model 2 — LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c0237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5297, number of negative: 3553456\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039320 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3500\n",
      "[LightGBM] [Info] Number of data points in the train set: 3558753, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001488 -> initscore=-6.508535\n",
      "[LightGBM] [Info] Start training from score -6.508535\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] PR-AUC=0.001385  LogLoss=0.014787  MAP@10=0.022670  NDCG@10=0.026775  HR@10=0.039892\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if HAS_LGB:\n",
    "    # Initialize LightGBM\n",
    "    lgbm = lgb.LGBMClassifier(\n",
    "        objective=\"binary\", learning_rate=0.05, n_estimators=2000,\n",
    "        num_leaves=63, min_child_samples=200,\n",
    "        subsample=0.9, colsample_bytree=0.9, reg_lambda=2.0,\n",
    "        scale_pos_weight=spw, random_state=42, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Train the model on the training subset\n",
    "    lgbm.fit(X_train, y_train,\n",
    "             eval_set=[(X_val, y_val)],\n",
    "             eval_metric=[\"average_precision\",\"binary_logloss\"],\n",
    "             callbacks=[lgb.early_stopping(200, verbose=False)])\n",
    "    \n",
    "    # Predict probabilities for the validation set\n",
    "    probs_lgb = lgbm.predict_proba(X_val)[:,1]\n",
    "\n",
    "    # Evaluate model performance on ranking + probability metrics and append results to the leaderboard list\n",
    "    results.append(evaluate_probs(\"LightGBM\", probs_lgb, y_val, val_df))\n",
    "else:\n",
    "    print(\"LightGBM not installed; skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c378ae",
   "metadata": {},
   "source": [
    "### Model 3 — XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075abc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XGBoost] PR-AUC=0.018531  LogLoss=0.032062  MAP@10=0.027957  NDCG@10=0.034063  HR@10=0.053358\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if HAS_XGB:\n",
    "    # Initialize XGBoost\n",
    "    xgb = XGBClassifier(\n",
    "        objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "        learning_rate=0.05, n_estimators=1200, max_depth=8,\n",
    "        subsample=0.9, colsample_bytree=0.9, reg_lambda=2.0,\n",
    "        scale_pos_weight=spw, random_state=42, n_jobs=-1, tree_method=\"hist\"\n",
    "    )\n",
    "\n",
    "    # Train the model on the training subset\n",
    "    xgb.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities for the validation set\n",
    "    probs_xgb = xgb.predict_proba(X_val)[:,1]\n",
    "\n",
    "    # Evaluate model performance on ranking + probability metrics and append results to the leaderboard list\n",
    "    results.append(evaluate_probs(\"XGBoost\", probs_xgb, y_val, val_df))\n",
    "else:\n",
    "    print(\"XGBoost not installed; skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f634d7a",
   "metadata": {},
   "source": [
    "## Leaderboard & position-blended MAP@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bec76883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>map10</th>\n",
       "      <th>ndcg10</th>\n",
       "      <th>hr10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.022005</td>\n",
       "      <td>0.028168</td>\n",
       "      <td>0.028708</td>\n",
       "      <td>0.033864</td>\n",
       "      <td>0.049992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.018531</td>\n",
       "      <td>0.032062</td>\n",
       "      <td>0.027957</td>\n",
       "      <td>0.034063</td>\n",
       "      <td>0.053358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.014787</td>\n",
       "      <td>0.022670</td>\n",
       "      <td>0.026775</td>\n",
       "      <td>0.039892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model    pr_auc   logloss     map10    ndcg10      hr10\n",
       "0  RandomForest  0.022005  0.028168  0.028708  0.033864  0.049992\n",
       "1       XGBoost  0.018531  0.032062  0.027957  0.034063  0.053358\n",
       "2      LightGBM  0.001385  0.014787  0.022670  0.026775  0.039892"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "leader = pd.DataFrame(results).sort_values([\"map10\",\"pr_auc\"], ascending=False).reset_index(drop=True)\n",
    "leader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca583d",
   "metadata": {},
   "source": [
    "Product rank in search results is already a very strong predictor\n",
    "(items shown higher are much more likely to be clicked/added).\n",
    "To leverage this, we blend the model’s predicted probability\n",
    "with a simple rank-based prior to improve MAP@K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf9760a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model=RandomForest  |  MAP@10 blended (α=0.7): 0.031544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optional: quick position blend for the top model in 'results'\n",
    "if results:\n",
    "    best = max(results, key=lambda r: (r[\"map10\"], r[\"pr_auc\"]))\n",
    "    best_name = best[\"model\"]\n",
    "    if best_name == \"RandomForest\":\n",
    "        probs = probs_rf\n",
    "    elif best_name == \"LightGBM\" and HAS_LGB:\n",
    "        probs = probs_lgb\n",
    "    elif best_name == \"XGBoost\" and HAS_XGB:\n",
    "        probs = probs_xgb\n",
    "    else:\n",
    "        probs = probs_rf\n",
    "\n",
    "    pos_prior = 1.0 / (1.0 + val_df[\"rank\"].values)\n",
    "    alpha = 0.7\n",
    "    blended = alpha*probs + (1-alpha)*pos_prior\n",
    "    ve = pd.DataFrame({\"search_query_id\": val_df[\"search_query_id\"].values,\n",
    "                       \"y_true\": y_val.values, \"y_score\": blended})\n",
    "    map10_blend = ve.groupby(\"search_query_id\").apply(ap_at_k, 10).mean()\n",
    "    print(f\"Best model={best_name}  |  MAP@10 blended (α=0.7): {map10_blend:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574011de",
   "metadata": {},
   "source": [
    "## Uplift\n",
    "\n",
    "I calculated uplift to translate model performance into business impact showing how much more effective we are at identifying add-to-bag events compared to random. I used Random Forest because it was the best-performing model in our evaluation, delivering the highest PR-AUC and MAP@10, making it the most reliable candidate for uplift analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ec42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uplift_table(y_true, y_score, n_bins=20):\n",
    "    \"\"\"\n",
    "    Build an uplift table by sorting on y_score (desc), splitting into n_bins equal-sized\n",
    "    groups (ventiles), and computing Class-1 rate and uplift vs the overall average.\n",
    "    \n",
    "    Returns:\n",
    "        table: DataFrame with per-ventile metrics (1 = top scores, n_bins = lowest)\n",
    "        summary: dict with overall_rate, uplift_top_5, uplift_top_10\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"y_true\": np.asarray(y_true).astype(int),\n",
    "                       \"y_score\": np.asarray(y_score).astype(float)}).copy()\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"Empty inputs.\")\n",
    "\n",
    "    # sort by score (highest first)\n",
    "    df = df.sort_values(\"y_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # assign ventiles by position (ensures exactly-equal sized bins when possible)\n",
    "    # ventile 1 = top 5%, ventile 20 = bottom 5%\n",
    "    idx = np.arange(1, n + 1)\n",
    "    df[\"ventile\"] = np.ceil(idx / (n / n_bins)).astype(int)\n",
    "    df[\"ventile\"] = df[\"ventile\"].clip(1, n_bins)\n",
    "\n",
    "    overall_rate = df[\"y_true\"].mean()\n",
    "\n",
    "    # per-ventile stats\n",
    "    g = df.groupby(\"ventile\", as_index=False).agg(\n",
    "        rows=(\"y_true\", \"size\"),\n",
    "        positives=(\"y_true\", \"sum\"),\n",
    "    )\n",
    "    g[\"class1_rate\"] = g[\"positives\"] / g[\"rows\"]\n",
    "    g[\"uplift_vs_overall\"] = np.where(\n",
    "        overall_rate > 0, g[\"class1_rate\"] / overall_rate, np.nan\n",
    "    )\n",
    "\n",
    "    # make ventile 1 the top bin in the display\n",
    "    g = g.sort_values(\"ventile\", ascending=True).reset_index(drop=True)\n",
    "    g[\"ventile_pct\"] = 100.0 / n_bins  # each bin is equal sized by construction\n",
    "\n",
    "    # cumulative (useful context)\n",
    "    g[\"cum_rows\"] = g[\"rows\"].cumsum()\n",
    "    g[\"cum_rows_pct\"] = 100 * g[\"cum_rows\"] / n\n",
    "    g[\"cum_positives\"] = g[\"positives\"].cumsum()\n",
    "    g[\"cum_class1_rate\"] = g[\"cum_positives\"] / g[\"cum_rows\"]\n",
    "    g[\"cum_uplift_vs_overall\"] = np.where(\n",
    "        overall_rate > 0, g[\"cum_class1_rate\"] / overall_rate, np.nan\n",
    "    )\n",
    "\n",
    "    # summary uplifts\n",
    "    top5 = g.loc[g[\"ventile\"] == 1, \"class1_rate\"].mean() if not g.empty else np.nan\n",
    "    top10 = g.loc[g[\"ventile\"].isin([1, 2]), \"positives\"].sum() / g.loc[g[\"ventile\"].isin([1, 2]), \"rows\"].sum()\n",
    "    uplift_top_5 = (top5 / overall_rate) if overall_rate > 0 else np.nan\n",
    "    uplift_top_10 = (top10 / overall_rate) if overall_rate > 0 else np.nan\n",
    "\n",
    "    summary = {\n",
    "        \"overall_rate\": overall_rate,\n",
    "        \"uplift_top_5pct\": uplift_top_5,   # ventile 1\n",
    "        \"uplift_top_10pct\": uplift_top_10, # ventiles 1–2\n",
    "    }\n",
    "    return g, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f599ce68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall add-to-bag rate: 0.001502\n",
      "Uplift (Top 5% vs overall): 3.957x\n",
      "Uplift (Top 10% vs overall): 2.272x\n",
      "   ventile  ventile_pct   rows  positives  class1_rate  uplift_vs_overall  \\\n",
      "0        1          5.0  33986        202     0.005944           3.956934   \n",
      "1        2          5.0  33986         30     0.000883           0.587663   \n",
      "2        3          5.0  33986         31     0.000912           0.607252   \n",
      "3        4          5.0  33987         42     0.001236           0.822705   \n",
      "4        5          5.0  33986         51     0.001501           0.999028   \n",
      "\n",
      "   cum_rows_pct  cum_class1_rate  cum_uplift_vs_overall  \n",
      "0      4.999963         0.005944               3.956934  \n",
      "1      9.999926         0.003413               2.272299  \n",
      "2     14.999890         0.002579               1.717283  \n",
      "3     20.000000         0.002244               1.493634  \n",
      "4     24.999963         0.002095               1.394713  \n"
     ]
    }
   ],
   "source": [
    "# Calculate uplift\n",
    "table, summ = uplift_table(y_val, probs_rf, n_bins=20)\n",
    "\n",
    "print(\"Overall add-to-bag rate:\", f\"{summ['overall_rate']:.6f}\")\n",
    "print(\"Uplift (Top 5% vs overall):\", f\"{summ['uplift_top_5pct']:.3f}x\")\n",
    "print(\"Uplift (Top 10% vs overall):\", f\"{summ['uplift_top_10pct']:.3f}x\")\n",
    "\n",
    "# Show the first few ventiles (top bins)\n",
    "display_cols = [\"ventile\", \"ventile_pct\", \"rows\", \"positives\",\n",
    "                \"class1_rate\", \"uplift_vs_overall\",\n",
    "                \"cum_rows_pct\", \"cum_class1_rate\", \"cum_uplift_vs_overall\"]\n",
    "print(table[display_cols].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847dc688",
   "metadata": {},
   "source": [
    "The Random Forest model is very effective at identifying a small slice of high-probability add-to-bag items. Targeting just the top 5% yields nearly 4× the conversion rate, making it a strong candidate for focused marketing or ranking use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a5e647",
   "metadata": {},
   "source": [
    "## Holdout predictions\n",
    "\n",
    "Generating the output predictions for hold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4089f594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best selected for full training: RandomForest\n",
      "Wrote predictions.csv\n"
     ]
    }
   ],
   "source": [
    "X_hold = holdout_df[FEATS]\n",
    "# Choose the best model from the earlier leaderboard; default to LightGBM if available\n",
    "best_model_name = leader.iloc[0][\"model\"] if len(leader) else (\"LightGBM\" if HAS_LGB else \"RandomForest\")\n",
    "print(\"Best selected for full training:\", best_model_name)\n",
    "\n",
    "hold_probs = rf.predict_proba(X_hold)[:,1]\n",
    "\n",
    "# Save predictions.csv with required columns\n",
    "submission = holdout.copy()\n",
    "submission[\"add_to_bag_probability\"] = np.clip(hold_probs, 1e-9, 1-1e-9)\n",
    "submission = submission[[\n",
    "    \"search_query_time\",\"user_id\",\"search_query_id\",\"product_id\",\n",
    "    \"rank\",\"price_discount_percentage\",\"add_to_bag_probability\"\n",
    "]]\n",
    "submission.to_csv(\"predictions.csv\", index=False)\n",
    "print(\"Wrote predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa8ec1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, I built models to predict the probability of a product being added to the shopping bag after appearing in search results. After evaluating Random Forest, LightGBM, and XGBoost, Random Forest performed best in terms of PR-AUC and MAP@10. To translate performance into business value, I calculated uplift, showing that the top 5% of predictions achieved an add-to-bag rate nearly 4× higher than average, demonstrating clear targeting power. These results confirm the model’s ability to meaningfully prioritize products with higher purchase intent.\n",
    "\n",
    "## Future work\n",
    "\n",
    "If more time and resources were available, I would:\n",
    "\n",
    "- **Enhance feature engineering**: include richer session features (e.g., number of products viewed in the same query, dwell time), user history (previous purchases, browsing patterns), and query semantics (NLP embeddings of search terms if available).\n",
    "\n",
    "- **Improve model tuning**: perform systematic hyperparameter optimization (e.g., Optuna, Bayesian optimization) for Random Forest, LightGBM, and XGBoost.\n",
    "\n",
    "- **Explore advanced models**: try CatBoost for better handling of categorical features, and neural network approaches for sequential/session modeling.\n",
    "\n",
    "- **Calibrate probabilities**: apply Platt scaling or isotonic regression to improve probability calibration, making outputs more actionable.\n",
    "\n",
    "- **Deploy and monito**r: integrate the model into a live pipeline, then monitor uplift stability across time, categories, and user segments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
